{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input: Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clusters for each component (fixed)\n",
    "num_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF: Load cleaned dataset from saved pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, do not perform cluster analysis for each component. \n",
    "# Perform analysis for the whole dataset. \n",
    "with open(\"cleaned_dummy_tfidf.pickle\", \"rb\") as pickle_file:\n",
    "    cleaned_data = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show dataset\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF: Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VECTORIZATION ###\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "message_list = list(cleaned_data['Requirement'])\n",
    "X = tfidf_vect.fit_transform(message_list)\n",
    "document_term_matrix = pd.DataFrame(X.toarray(),\n",
    "                                    columns=tfidf_vect.get_feature_names())\n",
    "vocabulary = tfidf_vect.vocabulary_\n",
    "vocabulary_freq = utils.count_vocab_freq(vocabulary, corpus=message_list)\n",
    "\n",
    "# View size of document term matrix for TF-IDF\n",
    "print(f'Document Term Matrix shape: {X.shape}')\n",
    "print(f'\\n')\n",
    "\n",
    "# View vocabulary list\n",
    "print(f'Vocabulary:')\n",
    "print(list(vocabulary_freq.items())[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF: Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word cloud of this vocabulary\n",
    "print('Dummy Dataset: Word Cloud \\n')\n",
    "dataset_word_cloud = WordCloud(width=3000, height=2000).generate_from_frequencies(vocabulary_freq)\n",
    "dataset_word_cloud.background_color = 'white'\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.axis('off')\n",
    "plt.imshow(dataset_word_cloud)\n",
    "# plt.savefig('dataset_word_cloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # up to 95% of total variance\n",
    "# pca = PCA(n_components=0.95, svd_solver='full')\n",
    "# prin_comp = pca.fit_transform(X.todense())\n",
    "# cumsum_variance = np.cumsum(pca.explained_variance_)\n",
    "\n",
    "# print(f'# of eigenvalues (principal components) needed to reach '\n",
    "#       f'{100*pca.n_components}% of explained variance: {pca.n_components_}')\n",
    "\n",
    "# print(f'Cumulative explained variance per principal component: '\n",
    "#       f'{cumsum_variance[:4]} ... {cumsum_variance[-4:]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF: K-Means for the Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K Means ###\n",
    "\n",
    "model = KMeans(n_clusters=num_clusters,\n",
    "               init='k-means++',\n",
    "               random_state=5).fit(X)\n",
    "sizes = np.array(np.unique(model.labels_,\n",
    "                           return_counts=True))[1]\n",
    "print('Cluster sizes (# of requirements per cluster): ', end=' ')\n",
    "print(sizes)\n",
    "\n",
    "cleaned_data['Predicted_Cluster_#'] = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Clusters_cleaned_dummy_tfidf.pickle\", \"wb\") as pickle_file:\n",
    "    pickle.dump(cleaned_data, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF: Inspect Requirements for Each Cluster\n",
    "\n",
    "**Note:** For visualization purposes, shuffle the rows in the dataset for each cluster using `pd.sample(frac=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reqs = cleaned_data[['Component/s', 'Requirement', 'Predicted_Cluster_#']]\n",
    "\n",
    "for k in range(num_clusters):\n",
    "    vocabulary = {}\n",
    "    vocabulary_freq = {}\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    message_list = list(cleaned_data['Description'].loc[cleaned_data['Predicted_Cluster_#'] == k])\n",
    "    message_list_preprocess = list(data_reqs['Requirement'].loc[data_reqs['Predicted_Cluster_#'] == k])\n",
    "    X = tfidf_vect.fit_transform(message_list_preprocess)\n",
    "    document_term_matrix = pd.DataFrame(X.toarray(), \n",
    "                                        columns=tfidf_vect.get_feature_names())\n",
    "    vocabulary[k] = tfidf_vect.vocabulary_\n",
    "    vocabulary_freq[k] = utils.count_vocab_freq(vocabulary[k],\n",
    "                                                corpus=message_list_preprocess)\n",
    "    # Word Cloud\n",
    "    word_cloud = WordCloud(width=3000, height=2000).generate_from_frequencies(vocabulary_freq[k])\n",
    "    word_cloud.background_color = 'white'\n",
    "\n",
    "    print(f\"Cluster #: {k}. Dimensions of TF-IDF matrix: {X.shape}\")\n",
    "    print(message_list[:3])\n",
    "    print(\"\\n\")\n",
    "    print(message_list_preprocess[:3])\n",
    "    print(\"\\n\")\n",
    "    print(f\"Vocab Frequency: {list(vocabulary_freq[k].items())[:30]}\")\n",
    "\n",
    "    plt.figure(figsize=[15,10])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(word_cloud)\n",
    "    print(\"\\n\")\n",
    "    print(data_reqs.loc[data_reqs['Predicted_Cluster_#'] == k].sample(frac=1))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec: Load cleaned dataset from saved pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, do not perform cluster analysis for each component. \n",
    "# Perform analysis for the whole dataset -- some test cases can span requirements from multiple system components. \n",
    "with open(\"cleaned_dummy_doc2vec.pickle\", \"rb\") as pickle_file:\n",
    "    cleaned_data_doc = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec: Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc to Vec\n",
    "import gensim\n",
    "\n",
    "def tagged_document(list_of_lists):\n",
    "    for i, list_of_words in enumerate(list_of_lists):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(tagged_document(cleaned_data_doc['Requirement']))\n",
    "d2v = gensim.models.doc2vec.Doc2Vec(vector_size=200, dm=0, min_count=2, epochs=50, seed=5)\n",
    "d2v.random.seed(5)\n",
    "d2v.build_vocab(corpus)\n",
    "d2v.random.seed(5)\n",
    "d2v.train(corpus, total_examples=d2v.corpus_count, epochs=d2v.epochs)\n",
    "\n",
    "# fit language model\n",
    "X = []\n",
    "for row in cleaned_data_doc['Requirement']:\n",
    "    d2v.random.seed(5)\n",
    "    X.append(d2v.infer_vector(row.split(' ')))\n",
    "\n",
    "cluster_X = np.array(X)\n",
    "print('Word embeddings shape: ', end=' ')\n",
    "print(cluster_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # up to 95% of total variance\n",
    "# pca = PCA(n_components=0.95, svd_solver='full')\n",
    "# prin_comp = pca.fit_transform(cluster_X.todense())\n",
    "# cumsum_variance = np.cumsum(pca.explained_variance_)\n",
    "\n",
    "# print(f'# of eigenvalues (principal components) needed to reach '\n",
    "#       f'{100*pca.n_components}% of explained variance: {pca.n_components_}')\n",
    "\n",
    "# print(f'Cumulative explained variance per principal component: '\n",
    "#       f'{cumsum_variance[:4]} ... {cumsum_variance[-4:]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec: K-Means for the Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K Means ###\n",
    "\n",
    "model = KMeans(n_clusters=num_clusters,\n",
    "               init='k-means++',\n",
    "               random_state=5).fit(cluster_X)\n",
    "sizes = np.array(np.unique(model.labels_,\n",
    "                           return_counts=True))[1]\n",
    "print('Cluster sizes (# of requirements per cluster): ', end=' ')\n",
    "print(sizes)\n",
    "\n",
    "cleaned_data_doc['Predicted_Cluster_#'] = model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec: Inspect Requirements for Each Cluster\n",
    "\n",
    "**Note:** For visualization purposes, shuffle the rows in the dataset for each cluster using `pd.sample(frac=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reqs = cleaned_data_doc[['Component/s', 'Requirement', 'Predicted_Cluster_#']]\n",
    "\n",
    "for k in range(num_clusters):\n",
    "    print(data_reqs.loc[data_reqs['Predicted_Cluster_#'] == k].sample(frac=1))\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
