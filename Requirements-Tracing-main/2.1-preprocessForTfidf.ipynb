{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d96e54",
   "metadata": {},
   "source": [
    "# Preprocessing \n",
    "\n",
    "All of these steps are optional. Finding the right way to preprocess your text data will be accomplished through trial and error. Additional, more targeted preprocessing may be necessary for each subset if you notice abnormalities that might affect your clustering (like ID numbers, specific formatting, etc.) My advice would be to use the Description and any other text data available to you if and when it makes sense to do so. The more text, the better, but only if it provides new and relevant information. In other words, don’t inject noise for the sake of adding more text data. \n",
    "\n",
    "Removing stop words (e.g., “the”, “and”, etc).  \n",
    "\n",
    "You can also create your own list of common words to remove if you find that your clusters are being impacted by words that aren’t really relevant (this will be revealed when you do key word extraction on your clusters). For example, you might find that you get clusters of tickets that all have vague words like “requirements” or “user.” While the language in those clusters is similar, it’s not really accomplishing the task we’re setting out to do, so you may want to consider removing these types of words from the text. You may also consider removing all Components and system names from the text. \n",
    "\n",
    "- Removing punctuation and/or digits \n",
    "\n",
    "- Lower casing \n",
    "\n",
    "- Lemmatizing  \n",
    "\n",
    "This will convert all words to their base form. For example, it would change “running” to “run” and “better” to “good.” This is more robust than stemming. \n",
    "\n",
    "If you’re using Doc2Vec or another algorithm that takes context into consideration, you may want to skip this step. Trial and error will help you make that decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e37c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bea777",
   "metadata": {},
   "source": [
    "# Import Processed Requirements Data from saved CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dummy_requirements.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53872a54",
   "metadata": {},
   "source": [
    "# Preprocess Text for NLP evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a599a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.astype(str)\n",
    "\n",
    "# ADD STOP WORDS HERE\n",
    "my_stop_words = ['the', 'system', 'shall', 'application', 'an', 'a', 'using', \n",
    "                 'standard', 'that', 'this']\n",
    "\n",
    "df['Requirement'] = df.apply(lambda x: utils.preprocess_text(x['Description'], my_stop_words, \n",
    "                                                             keep_original=False, display=False, \n",
    "                                                             pos_tags=True), \n",
    "                                                             axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POS'] = df.apply(lambda x: x['Requirement'][1], axis=1)\n",
    "df['Requirement'] = df.apply(lambda x: x['Requirement'][0],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_dummy_tfidf.pickle\", \"wb\") as pickle_file:\n",
    "    pickle.dump(df, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3aa8c",
   "metadata": {},
   "source": [
    "# View list of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954aac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Requirements by Component/Program of Record\n",
    "components = df.groupby(by='Component/s')\n",
    "components.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59375a08",
   "metadata": {},
   "source": [
    "# Subgroup by component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = components.groups.keys()\n",
    "grouped_data = {}\n",
    "for g in groups:\n",
    "    print(g)\n",
    "    sub_df = components.get_group(g)\n",
    "    sub_df = sub_df.astype(str)\n",
    "    X = sub_df\n",
    "    grouped_data[g] = X\n",
    "    print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show output\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DummyPreproccessedForTfidf.pickle\", \"wb\") as pickle_file:\n",
    "    pickle.dump(grouped_data, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d47c5",
   "metadata": {},
   "source": [
    "## Legend for Parts of Speech (POS): \n",
    "\n",
    "- CC coordinating conjunction\n",
    "- CD cardinal digit\n",
    "- DT determiner\n",
    "- EX existential there (like: “there is” … think of it like “there exists”)\n",
    "- FW foreign word\n",
    "- IN preposition/subordinating conjunction\n",
    "- JJ adjective ‘big’\n",
    "- JJR adjective, comparative ‘bigger’\n",
    "- JJS adjective, superlative ‘biggest’\n",
    "- LS list marker 1)\n",
    "- MD modal could, will\n",
    "- NN noun, singular ‘desk’\n",
    "- NNS noun plural ‘desks’\n",
    "- NNP proper noun, singular ‘Harrison’\n",
    "- NNPS proper noun, plural ‘Americans’\n",
    "- PDT predeterminer ‘all the kids’\n",
    "- POS possessive ending parent‘s\n",
    "- PRP personal pronoun I, he, she\n",
    "- PRPS possessive pronoun my, his, hers\n",
    "- RB adverb very, silently,\n",
    "- RBR adverb, comparative better\n",
    "- RBS adverb, superlative best\n",
    "- RP particle give up\n",
    "- TO to go ‘to‘ the store.\n",
    "- UH interjection errrrrrrrm\n",
    "- VB verb, base form take\n",
    "- VBD verb, past tense took\n",
    "- VBG verb, gerund/present participle taking\n",
    "- VBN verb, past participle taken\n",
    "- VBP verb, sing. present, non-3d take\n",
    "- VBZ verb, 3rd person sing. present takes\n",
    "- WDT wh-determiner which\n",
    "- WP wh-pronoun who, what\n",
    "- WPS possessive wh-pronoun whose\n",
    "- WRB wh-abverb where, when"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
