{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing \n",
    "\n",
    "All of these steps are optional. Finding the right way to preprocess your text data will be accomplished through trial and error. Additional, more targeted preprocessing may be necessary for each subset if you notice abnormalities that might affect your clustering (like ID numbers, specific formatting, etc.) My advice would be to use the Description and any other text data available to you if and when it makes sense to do so. The more text, the better, but only if it provides new and relevant information. In other words, don’t inject noise for the sake of adding more text data. \n",
    "\n",
    "Removing stop words, e.g. “the”, “and”, etc.  \n",
    "\n",
    "You can also create your own list of common words to remove if you find that your clusters are being impacted by words that aren’t really relevant (this will be revealed when you do key word extraction on your clusters). For example, you might find that you get clusters of tickets that all have vague words like “requirements” or “user.” While the language in those clusters is similar, it’s not really accomplishing the task we’re setting out to do, so you may want to consider removing these types of words from the text. You may also consider removing all Components and system names from the text. \n",
    "\n",
    "Removing punctuation and/or digits \n",
    "\n",
    "Lower casing \n",
    "\n",
    "Lemmatizing  \n",
    "\n",
    "This will convert all words to their base form. For example, it would change “running” to “run” and “better” to “good.” This is more robust than stemming. \n",
    "\n",
    "If you’re using Doc2Vec or another algorithm that takes context into consideration, you may want to skip this step. Trial and error will help you make that decision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Processed Requirements Data from saved CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dummy_requirements.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text for NLP evaluation\n",
    "## minimal preprocessing for Doc2Vec Vectorization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(str)\n",
    "my_stop_words = [] # minimal preprocessing. do not remove stopwords or lemmatize for doc2vec\n",
    "\n",
    "df['Requirement'] = df.apply(lambda x: utils.preprocess_text(x['Description'], my_stop_words,\n",
    "                                                             keep_original=True, display=False, \n",
    "                                                             pos_tags=True), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment after debugging POS - will need to pull POS field out of Requirement field...\n",
    "df['POS'] = df.apply(lambda x: x['Requirement'][1], axis=1)\n",
    "df['Requirement'] = df.apply(lambda x: x['Requirement'][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_dummy_doc2vec.pickle\", \"wb\") as pickle_file:\n",
    "    pickle.dump(df, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View list of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Requirements by Component/Program of Record\n",
    "components = df.groupby(by='Component/s')\n",
    "components.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subgroup by component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = components.groups.keys()\n",
    "grouped_data = {}\n",
    "for g in groups:\n",
    "    print(g)\n",
    "    sub_df = components.get_group(g)\n",
    "    sub_df = sub_df.astype(str)\n",
    "    X = sub_df\n",
    "    grouped_data[g] = X\n",
    "    print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DummyPreproccessedForDoc2Vec.pickle\", \"wb\") as pickle_file:\n",
    "    pickle.dump(grouped_data, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e0508b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
