{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input: Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clusters for each component (fixed)\n",
    "# Note: There are 19 unique component/s for this dataset. \n",
    "num_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load grouped data by component from saved pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DummyPreproccessedForDoc2Vec.pickle\", \"rb\") as pickle_file:\n",
    "    grouped_data = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine by Subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select MCH subgroup to perform TF-IDF vectorization and k-means clustering\n",
    "groups = grouped_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization: Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Doc to Vec\n",
    "import gensim\n",
    "import numpy as np\n",
    "def tagged_document(list_of_lists):\n",
    "    for i, list_of_words in enumerate(list_of_lists):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "        \n",
    "# corpus = list(tagged_document(df['token']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_X = {}\n",
    "for g in groups:\n",
    "    corpus = list(tagged_document(grouped_data[g]['Requirement']))\n",
    "    d2v = gensim.models.doc2vec.Doc2Vec(vector_size=200, dm=0, min_count=2, epochs=50, seed=5)\n",
    "    d2v.random.seed(5)\n",
    "    d2v.build_vocab(corpus)\n",
    "    d2v.random.seed(5)\n",
    "    d2v.train(corpus, total_examples=d2v.corpus_count, epochs=d2v.epochs)\n",
    "    # fit language model\n",
    "    X = []\n",
    "    for row in grouped_data[g]['Requirement']:\n",
    "        d2v.random.seed(5)\n",
    "        X.append(d2v.infer_vector(row.split(' ')))\n",
    "\n",
    "    X = np.array(X)\n",
    "    group_X[g] = X\n",
    "    print('Word embeddings shape: ', end=' ')\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_X\n",
    "with open(\"VectorizationDoc2Vec.pickle\", \"wb\") as pickle_file:\n",
    "    pickle.dump(group_X, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for component_name in list(groups):\n",
    "#     # up to 95% of total variance\n",
    "#     pca = PCA(n_components=0.95, svd_solver='full')\n",
    "#     prin_comp = pca.fit_transform(group_X[component_name])\n",
    "#     cumsum_variance = np.cumsum(pca.explained_variance_)\n",
    "    \n",
    "#     print(f'Component Name = {component_name}:')\n",
    "\n",
    "#     print(f'# of eigenvalues (principal components) needed to reach '\n",
    "#           f'{100*pca.n_components}% of explained variance: {pca.n_components_}')\n",
    "\n",
    "#     print(f'Cumulative explained variance per principal component: '\n",
    "#           f'{cumsum_variance[:4]} ... {cumsum_variance[-4:]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means on all Subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cluster\n",
    "'''\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "for g in groups:\n",
    "    \n",
    "    #FIXME: Each subgroup will have a different optimal K clusters... \n",
    "    # num_clusters = 6 # defined on top cell\n",
    "    if len(grouped_data[g]) < num_clusters:\n",
    "        num_clusters = 1\n",
    "        \n",
    "    \n",
    "    model = KMeans(n_clusters=num_clusters, init='k-means++', random_state=5).fit(group_X[g])\n",
    "    sizes = np.array(np.unique(model.labels_, return_counts=True))[1]\n",
    "    print('Cluster sizes: ', end=' ')\n",
    "    print(sizes)\n",
    "\n",
    "    grouped_data[g]['predicted'] = model.labels_\n",
    "    grouped_data[g].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ClustersDoc2Vec.pickle\", \"wb\") as pickle_file:\n",
    "    pickle.dump(grouped_data, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
