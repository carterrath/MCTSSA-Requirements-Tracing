{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input: Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clusters for each component (fixed)\n",
    "num_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load grouped data by component from saved pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DummyPreproccessedForTfidf.pickle\", \"rb\") as pickle_file:\n",
    "    grouped_data = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine ALPHA Subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select ALPHA subgroup to perform TF-IDF vectorization and k-means clustering\n",
    "groups = ['ALPHA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### VECTORIZATION ###\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vocabulary = {}\n",
    "vocabulary_freq = {}\n",
    "for g in groups:\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    message_list = list(grouped_data[g]['Requirement'])\n",
    "    X = tfidf_vect.fit_transform(message_list)\n",
    "    document_term_matrix = pd.DataFrame(X.toarray(), \n",
    "                                        columns=tfidf_vect.get_feature_names())\n",
    "    vocabulary[g] = tfidf_vect.vocabulary_\n",
    "    vocabulary_freq[g] = utils.count_vocab_freq(vocabulary[g], \n",
    "                                                corpus=message_list)\n",
    "    print(g, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view vocabulary for subgroup component ALPHA\n",
    "list(vocabulary_freq.items())[:50]\n",
    "\n",
    "# Suggestion:  Look into acronmyms used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be186e6c",
   "metadata": {},
   "source": [
    "# Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1219da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word cloud of this vocabulary\n",
    "print('One System: Word Cloud \\n')\n",
    "one_system_word_cloud = WordCloud(width=3000, height=2000).generate_from_frequencies(vocabulary_freq[g])\n",
    "one_system_word_cloud.background_color = 'white'\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.axis('off')\n",
    "plt.imshow(one_system_word_cloud)\n",
    "# plt.savefig('one_system_word_cloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for component_name in list(groups):\n",
    "#     # up to 95% of total variance\n",
    "#     pca = PCA(n_components=0.95, svd_solver='full')\n",
    "#     prin_comp = pca.fit_transform(X.todense())\n",
    "#     cumsum_variance = np.cumsum(pca.explained_variance_)\n",
    "    \n",
    "#     print(f'Component Name = {component_name}:')\n",
    "\n",
    "#     print(f'# of eigenvalues (principal components) needed to reach '\n",
    "#           f'{100*pca.n_components}% of explained variance: {pca.n_components_}')\n",
    "\n",
    "#     print(f'Cumulative explained variance per principal component: '\n",
    "#           f'{cumsum_variance[:4]} ... {cumsum_variance[-4:]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means on Subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K Means ###\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# num_clusters = 6 # number of labels in eval_df\n",
    "model = KMeans(n_clusters = num_clusters, init = 'k-means++', random_state = 5).fit(X)\n",
    "sizes = np.array(np.unique(model.labels_, return_counts = True))[1]\n",
    "print('Cluster sizes: ', end=' ')\n",
    "print(sizes)\n",
    "\n",
    "grouped_data[g]['predicted'] = model.labels_\n",
    "grouped_data[g].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize: What's in the clusters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show first 3 requirements in each cluster, plus the word cloud\n",
    "for count in range(num_clusters):\n",
    "    vocabulary = {}\n",
    "    vocabulary_freq = {}\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    message_list = list(grouped_data[g]['Description'].loc[grouped_data[g]['predicted'] == count])\n",
    "    message_list_preprocess = list(grouped_data[g]['Requirement'].loc[grouped_data[g]['predicted'] == count])\n",
    "    X = tfidf_vect.fit_transform(message_list_preprocess)\n",
    "    document_term_matrix = pd.DataFrame(X.toarray(), \n",
    "                                        columns=tfidf_vect.get_feature_names())\n",
    "    vocabulary[count] = tfidf_vect.vocabulary_\n",
    "    vocabulary_freq[count] = utils.count_vocab_freq(vocabulary[count],\n",
    "                                                    corpus=message_list_preprocess)\n",
    "    # Word Cloud\n",
    "    word_cloud = WordCloud(width=3000, height=2000).generate_from_frequencies(vocabulary_freq[count])\n",
    "    word_cloud.background_color = 'white'\n",
    "\n",
    "    print(f\"Cluster #: {count}. Dimensions of TF-IDF matrix: {X.shape}\")\n",
    "    print(message_list[:3])\n",
    "    print(\"\\n\")\n",
    "    print(message_list_preprocess[:3])\n",
    "    print(\"\\n\")\n",
    "    print(f\"Vocab Frequency: {list(vocabulary_freq[count].items())[:30]}\")\n",
    "\n",
    "    plt.figure(figsize=[15,10])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(word_cloud)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### VISUALIZE ### What requirements are in the cluster (first 20 only)?\n",
    "\n",
    "# Update the following to inspect the clusters\n",
    "view = 'Requirement' # Uncomment to view prepocessed message \n",
    "# view = 'Summary' # Uncomment to view original summary message \n",
    "\n",
    "for cluster_number in range(num_clusters):\n",
    "    print(f'Component Name: {g}')\n",
    "    print(f'Cluster Number: {cluster_number}')\n",
    "    print(f\"{list(grouped_data[g][view].loc[grouped_data[g]['predicted'] == cluster_number].head(20))} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to determine optimal K clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Determining optimal K clusters ###\n",
    "# Uses elbow method and silhouette coefficient\n",
    "# This takes a while...\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "silhouette_coefficients = []\n",
    "Sum_of_squared_distances = []\n",
    "K = range(2, 100)\n",
    "kmeans_kwargs = {\n",
    "         \"init\": \"random\",\n",
    "         \"n_init\": 10,\n",
    "         \"max_iter\": 300,\n",
    "         \"random_state\": 42,\n",
    "     }\n",
    "for k in K: \n",
    "    km = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    km = km.fit(X)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "#     print(km.labels_)\n",
    "    score = silhouette_score(X, km.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(K,Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(K, silhouette_coefficients)\n",
    "plt.xticks(K)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_coefficients\n",
    "\n",
    "max_value = max(silhouette_coefficients)\n",
    "max_index = silhouette_coefficients.index(max_value)\n",
    "print(max_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using YAKE for Keyword Extraction for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clusters_data to more easily access the requirements in each cluster\n",
    "clusters_data = []\n",
    "for cluster_number in range(max_index):\n",
    "    clusters_data.append(list(grouped_data[g]['Requirement'].loc[grouped_data[g]['predicted']==cluster_number]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Yake to on clusters and save keywords for each cluster\n",
    "\n",
    "import yake\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 50\n",
    "yake_keywords_clusters = {}\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, \n",
    "                                            n=max_ngram_size, \n",
    "                                            dedupLim=deduplication_thresold, \n",
    "                                            dedupFunc=deduplication_algo, \n",
    "                                            windowsSize=windowSize, \n",
    "                                            top=numOfKeywords, \n",
    "                                            features=None)\n",
    "for cluster_number in range(max_index):\n",
    "    print(cluster_number)\n",
    "    keywords = []\n",
    "    for text in clusters_data[cluster_number]:\n",
    "        kw = custom_kw_extractor.extract_keywords(text)\n",
    "        keywords.append(' '.join([word for word, score in kw]))\n",
    "    yake_keywords_clusters[cluster_number] = kw\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english', ngram_range=(0,1))\n",
    "    X = vectorizer.fit_transform(keywords)\n",
    "    ## X will be array of word embeddings\n",
    "#     print('Word embeddings shape: ', end=' ')\n",
    "#     print(cluster_number, X.shape)\n",
    "    document_term_matrix = pd.DataFrame(X.toarray(),\n",
    "                                        columns=vectorizer.get_feature_names())\n",
    "    \n",
    "# with open(\"yake_vocabulary.txt\", \"w\") as text_file:\n",
    "#     text_file.write(str(yake_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yake_keywords_clusters[0]\n",
    "# TODO: for each cluster, change list to dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
