{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input: Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clusters for each component (fixed)\n",
    "# Note: There are 19 unique component/s for this dataset. \n",
    "num_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load grouped data by component from saved pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DummyPreproccessedForDoc2Vec.pickle\", \"rb\") as pickle_file:\n",
    "    grouped_data = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine by Subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subgroup to perform Doc2Vec vectorization and k-means clustering\n",
    "groups = ['ALPHA']\n",
    "# groups = grouped_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization: Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Doc to Vec\n",
    "import gensim\n",
    "\n",
    "def tagged_document(list_of_lists):\n",
    "    for i, list_of_words in enumerate(list_of_lists):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_X = {}\n",
    "\n",
    "for g in groups:\n",
    "    corpus = list(tagged_document(grouped_data[g]['Requirement']))\n",
    "    d2v = gensim.models.doc2vec.Doc2Vec(vector_size=200, dm=0, min_count=2, epochs=50, seed=5)\n",
    "    d2v.random.seed(5)\n",
    "    d2v.build_vocab(corpus)\n",
    "    d2v.random.seed(5)\n",
    "    d2v.train(corpus, total_examples=d2v.corpus_count, epochs=d2v.epochs)\n",
    "    # fit language model\n",
    "    X = []\n",
    "    for row in grouped_data[g]['Requirement']:\n",
    "        d2v.random.seed(5)\n",
    "        X.append(d2v.infer_vector(row.split(' ')))\n",
    "\n",
    "    X = np.array(X)\n",
    "    cluster_X[g] = X\n",
    "    print('Word embeddings shape: ', end=' ')\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for component_name in list(groups):\n",
    "#     # up to 95% of total variance\n",
    "#     pca = PCA(n_components=0.95, svd_solver='full')\n",
    "#     prin_comp = pca.fit_transform(cluster_X[component_name])\n",
    "#     cumsum_variance = np.cumsum(pca.explained_variance_)\n",
    "    \n",
    "#     print(f'Component Name = {component_name}:')\n",
    "\n",
    "#     print(f'# of eigenvalues (principal components) needed to reach '\n",
    "#           f'{100*pca.n_components}% of explained variance: {pca.n_components_}')\n",
    "\n",
    "#     print(f'Cumulative explained variance per principal component: '\n",
    "#           f'{cumsum_variance[:4]} ... {cumsum_variance[-4:]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means on one Subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for g in groups:\n",
    "    \n",
    "    #FIXME: Each subgroup will have a different optimal K clusters... \n",
    "    # num_clusters = 6  \n",
    "    if len(grouped_data[g]) < num_clusters:\n",
    "        num_clusters = 1\n",
    "        \n",
    "    \n",
    "    model = KMeans(n_clusters=num_clusters, init='k-means++', random_state=5).fit(cluster_X[g])\n",
    "    sizes = np.array(np.unique(model.labels_, return_counts=True))[1]\n",
    "    print('Cluster sizes: ', end=' ')\n",
    "    print(sizes)\n",
    "\n",
    "    grouped_data[g]['Predicted_Cluster_#'] = model.labels_\n",
    "    print(grouped_data[g][0:len(model.labels_):500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec: Inspect Requirements for Each Cluster\n",
    "\n",
    "**Note:** For visualization purposes, shuffle the rows in the dataset for each cluster using `pd.sample(frac=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show first 5 requirements in each cluster\n",
    "\n",
    "for g in groups:\n",
    "    print(g)\n",
    "    for count in range(num_clusters):\n",
    "        print(count)\n",
    "        print(list(grouped_data[g]['Requirement'].loc[grouped_data[g]['Predicted_Cluster_#'] == count].head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### VISUALIZE ### What requirements are in the cluster?\n",
    "\n",
    "# Update the following to inspect the clusters\n",
    "view = 'Requirement' # Uncomment to view prepocessed message \n",
    "# view = 'Summary' # Uncomment to view original summary message \n",
    "\n",
    "for cluster_number in range(num_clusters):\n",
    "    print(f'Component Name: {g}')\n",
    "    print(f'Cluster Number: {cluster_number}')\n",
    "    print(f\"{list(grouped_data[g][view].loc[grouped_data[g]['Predicted_Cluster_#'] == cluster_number].head(20))} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine optimal K clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Determining optimal K clusters ###\n",
    "# Uses elbow method and silhouette coefficient\n",
    "# This takes a while...\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def determine_optimal_k(K):\n",
    "    silhouette_coefficients = []\n",
    "    Sum_of_squared_distances = []\n",
    "    kmeans_kwargs = {\n",
    "             \"init\": \"random\",\n",
    "             \"n_init\": 10,\n",
    "             \"max_iter\": 300,\n",
    "             \"random_state\": 42,\n",
    "         }\n",
    "    for k in K: \n",
    "        km = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        km = km.fit(X)\n",
    "        Sum_of_squared_distances.append(km.inertia_)\n",
    "    #     print(km.labels_)\n",
    "        score = silhouette_score(X, km.labels_)\n",
    "        silhouette_coefficients.append(score)\n",
    "\n",
    "\n",
    "    plt.plot(K,Sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    plt.plot(K, silhouette_coefficients)\n",
    "    plt.xticks(K)\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Silhouette Coefficient\")\n",
    "    plt.show()\n",
    "    \n",
    "    max_value = max(silhouette_coefficients)\n",
    "    max_index = silhouette_coefficients.index(max_value)\n",
    "    print('max_index',max_index)\n",
    "    return max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Changed range(2, 100) -> range(2, 87)\n",
    "K = range(2, 100)\n",
    "max_index = determine_optimal_k(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clusters_data to more easily access the requirements in each cluster\n",
    "clusters_data = []\n",
    "for cluster_number in range(max_index):\n",
    "    clusters_data.append(list(grouped_data[g]['Requirement'].loc[grouped_data[g]['Predicted_Cluster_#'] == cluster_number]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Yake to on clusters and save keywords for each cluster\n",
    "\n",
    "import yake\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 10\n",
    "yake_keywords_clusters = {}\n",
    "for cluster_number in range(max_index):\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, \n",
    "                                            n=max_ngram_size, \n",
    "                                            dedupLim=deduplication_thresold, \n",
    "                                            dedupFunc=deduplication_algo, \n",
    "                                            windowsSize=windowSize, \n",
    "                                            top=numOfKeywords, \n",
    "                                            features=None)\n",
    "    keywords = []\n",
    "    for text in clusters_data[cluster_number]:\n",
    "        kw = custom_kw_extractor.extract_keywords(text)\n",
    "        keywords.append(' '.join([word for word, score in kw]))\n",
    "    yake_keywords_clusters[cluster_number] = kw\n",
    "\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english', ngram_range=(0,1))\n",
    "    X = vectorizer.fit_transform(keywords)\n",
    "    ## X will be array of word embeddings\n",
    "#     print('Word embeddings shape: ', end=' ')\n",
    "#     print(cluster_number, X.shape)\n",
    "    document_term_matrix = pd.DataFrame(X.toarray(), \n",
    "                                        columns=vectorizer.get_feature_names())\n",
    "    \n",
    "# with open(\"yake_vocabulary.txt\", \"w\") as text_file:\n",
    "#     text_file.write(str(yake_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yake_keywords_clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
