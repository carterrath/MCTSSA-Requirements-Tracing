{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input: Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clusters\n",
    "num_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dochr\\Desktop\\Jupyter Notebooks - Requirements\\5-InspectClusters.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dochr/Desktop/Jupyter%20Notebooks%20-%20Requirements/5-InspectClusters.ipynb#ch0000002?line=0'>1</a>\u001b[0m \u001b[39m# import packages\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dochr/Desktop/Jupyter%20Notebooks%20-%20Requirements/5-InspectClusters.ipynb#ch0000002?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dochr/Desktop/Jupyter%20Notebooks%20-%20Requirements/5-InspectClusters.ipynb#ch0000002?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dochr/Desktop/Jupyter%20Notebooks%20-%20Requirements/5-InspectClusters.ipynb#ch0000002?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\__init__.py:135\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=116'>117</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomputation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39meval\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=118'>119</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=119'>120</a>\u001b[0m     concat,\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=120'>121</a>\u001b[0m     lreshape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=131'>132</a>\u001b[0m     qcut,\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=132'>133</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=134'>135</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m api, arrays, errors, io, plotting, testing, tseries\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=135'>136</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_print_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=137'>138</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=138'>139</a>\u001b[0m     \u001b[39m# excel\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=139'>140</a>\u001b[0m     ExcelFile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=167'>168</a>\u001b[0m     read_spss,\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/__init__.py?line=168'>169</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\testing.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=0'>1</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=1'>2</a>\u001b[0m \u001b[39mPublic testing utility functions.\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=2'>3</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m----> <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_testing\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=6'>7</a>\u001b[0m     assert_extension_array_equal,\n\u001b[0;32m      <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=7'>8</a>\u001b[0m     assert_frame_equal,\n\u001b[0;32m      <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=8'>9</a>\u001b[0m     assert_index_equal,\n\u001b[0;32m     <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=9'>10</a>\u001b[0m     assert_series_equal,\n\u001b[0;32m     <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=10'>11</a>\u001b[0m )\n\u001b[0;32m     <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=12'>13</a>\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=13'>14</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_extension_array_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=14'>15</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_frame_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=15'>16</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_series_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=16'>17</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_index_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/testing.py?line=17'>18</a>\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\_testing\\__init__.py:974\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/_testing/__init__.py?line=968'>969</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpytest\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/_testing/__init__.py?line=970'>971</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pytest\u001b[39m.\u001b[39mraises(expected_exception, match\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)  \u001b[39m# noqa: PDF010\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/_testing/__init__.py?line=973'>974</a>\u001b[0m cython_table \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mcore\u001b[39m.\u001b[39mcommon\u001b[39m.\u001b[39m_cython_table\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/_testing/__init__.py?line=976'>977</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cython_table_params\u001b[39m(ndframe, func_names_and_expected):\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/_testing/__init__.py?line=977'>978</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/_testing/__init__.py?line=978'>979</a>\u001b[0m \u001b[39m    Combine frame, functions from com._cython_table\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/_testing/__init__.py?line=979'>980</a>\u001b[0m \u001b[39m    keys and expected result.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/_testing/__init__.py?line=991'>992</a>\u001b[0m \u001b[39m        List of three items (DataFrame, function, expected result)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/dochr/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/pandas/_testing/__init__.py?line=992'>993</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import utils\n",
    "import pickle\n",
    "import yake\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grouped data by component from saved pickle\n",
    "\n",
    "with open(\"ClustersTfidf.pickle\", \"rb\") as pickle_file:\n",
    "    grouped_data = pickle.load(pickle_file)\n",
    "\n",
    "# #Uncomment to inspect Doc2Vec    \n",
    "# with open(\"ClustersDoc2Vec.pickle\", \"rb\") as pickle_file:\n",
    "#     grouped_data = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine by Subgroup\n",
    "groups = grouped_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize: What's in the clusters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show first 5 requirements in each cluster\n",
    "for g in groups:\n",
    "    print(g)\n",
    "    for count in range(num_clusters):\n",
    "        print(utils.inspect_cluster(grouped_data, g, count)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### VISUALIZE ### What requirements are in the cluster?\n",
    "\n",
    "# Update the following to inspect the clusters\n",
    "cluster_number = 3 # change this to view different clusters\n",
    "view = 'Requirement' # Uncomment to view prepocessed message \n",
    "# view = 'Summary' # Uncomment to view original summary message \n",
    "\n",
    "utils.inspect_cluster(grouped_data, g, cluster_number)\n",
    "\n",
    "# list(grouped_data[g][view].loc[grouped_data[g]['predicted']==cluster_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Descriptions\n",
    "## Keyword Extraction Using YAKE\n",
    "You can get keywords to represent your clusters by using keyword extraction. YAKE is a good one, and it’s easy to use. First, you’ll need to convert your vector back to its original text. If you didn’t lemmatize during your preprocessing, it’s a good idea to do that before applying keyword extraction. Then, combine all the text in your cluster into a single text blob or “document,” meaning, you’ll need to collapse your list of lists into a single element. Then, simply use YAKE or another keyword extractor to get the most representative words for your cluster. If you don’t see anything meaningful here, your clustering algorithm may be picking up on patterns you don’t care about. Add any irrelevant words to your list of words to remove and try again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clusters_data to more easily access the requirements in each cluster\n",
    "clusters_data = []\n",
    "max_index = 4\n",
    "for cluster_number in range(max_index):\n",
    "    clusters_data.append(list(grouped_data[g]['Requirement'].loc[grouped_data[g]['predicted']==cluster_number]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attempting to use Erin's code to print out keywords with POS - needs debugging\n",
    "# ### Using Yake to to vectorize and save vocab for each group\n",
    "# yake_vocab = {}\n",
    "# language = \"en\"\n",
    "# max_ngram_size = 1\n",
    "# deduplication_thresold = 0.9\n",
    "# deduplication_algo = 'seqm'\n",
    "# windowSize = 1\n",
    "# numOfKeywords = 5 # changed to 5 so it doesn't return every word in the Requirement\n",
    "\n",
    "# ### pull this out of the for loop for efficiency\n",
    "# custom_kw_extractor = yake.KeywordExtractor(lan=language, \n",
    "#                                         n=max_ngram_size, \n",
    "#                                         dedupLim=deduplication_thresold, \n",
    "#                                         dedupFunc=deduplication_algo, \n",
    "#                                         windowsSize=windowSize, \n",
    "#                                         top=numOfKeywords, \n",
    "#                                         features=None)\n",
    "\n",
    "# vocab = {}\n",
    "# for g in groups:\n",
    "#     vocab[g] = []\n",
    "#     keywords = []\n",
    "#     tags = []\n",
    "#     for i, text in enumerate(grouped_data[g]['Requirement']):\n",
    "#         kw = custom_kw_extractor.extract_keywords(text)\n",
    "# #         keywords.append(' '.join([word for word, score in kw]))\n",
    "#         keywords.append([word for word, score in kw])\n",
    "#         text = nltk.word_tokenize(text)\n",
    "        \n",
    "#         tmp_tags = []\n",
    "#         for keyword in [word for word, score in kw]:\n",
    "#             try:\n",
    "#                 tmp_tags.append(grouped_data[g].iloc[i]['tags'][text.index(keyword)])\n",
    "#             except:\n",
    "#                 continue\n",
    "#         tags.append(tmp_tags)  \n",
    "#     for i in range(len(keywords)):\n",
    "#         new_requirement = []\n",
    "#         for j in range(len(keywords[i])):\n",
    "#             try:\n",
    "#                 new_requirement.append((keywords[i][j], tags[i][j]))\n",
    "#             except:\n",
    "#                 print(g, i, j)\n",
    "#                 print(keywords[i], tags[i])\n",
    "#         vocab[g].append(new_requirement)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Yake to on clusters and save keywords for each cluster\n",
    "\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 4\n",
    "yake_keywords_clusters = {}\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, \n",
    "                                            n=max_ngram_size, \n",
    "                                            dedupLim=deduplication_thresold, \n",
    "                                            dedupFunc=deduplication_algo, \n",
    "                                            windowsSize=windowSize, \n",
    "                                            top=numOfKeywords, \n",
    "                                            features=None)\n",
    "for cluster_number in range(max_index):\n",
    "    keywords = []\n",
    "    for text in clusters_data[cluster_number]:\n",
    "        kw = custom_kw_extractor.extract_keywords(text)\n",
    "        keywords.append(' '.join([word for word, score in kw]))\n",
    "    yake_keywords_clusters[cluster_number] = kw\n",
    "\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english', ngram_range=(0,1))\n",
    "    X = vectorizer.fit_transform(keywords)\n",
    "    ## X will be array of word embeddings\n",
    "#     print('Word embeddings shape: ', end=' ')\n",
    "#     print(cluster_number, X.shape)\n",
    "    document_term_matrix = pd.DataFrame(X.toarray(),\n",
    "                                        columns=vectorizer.get_feature_names())\n",
    "    \n",
    "# with open(\"yake_vocabulary.txt\", \"w\") as text_file:\n",
    "#     text_file.write(str(yake_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yake_keywords_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Yake to to vectorize and save vocab for each group\n",
    "# yake_vocab={}\n",
    "\n",
    "# custom_kw_extractor = yake.KeywordExtractor(lan=language, \n",
    "#                                             n=max_ngram_size, \n",
    "#                                             dedupLim=deduplication_thresold, \n",
    "#                                             dedupFunc=deduplication_algo, \n",
    "#                                             windowsSize=windowSize, \n",
    "#                                             top=numOfKeywords, \n",
    "#                                             features=None)\n",
    "# for g in groups:\n",
    "#     \n",
    "#     keywords = []\n",
    "#     for text in grouped_data[g]['preprocessed']:\n",
    "#         kw = custom_kw_extractor.extract_keywords(text)\n",
    "#         keywords.append(' '.join([word for word, score in kw]))\n",
    "\n",
    "#     vectorizer = CountVectorizer(stop_words='english', ngram_range=(0,1))\n",
    "#     X = vectorizer.fit_transform(keywords)\n",
    "#     ## X will be array of word embeddings\n",
    "# #     print('Word embeddings shape: ', end=' ')\n",
    "#     print(g, X.shape)\n",
    "#     document_term_matrix=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "#     yake_vocab[g]=vectorizer.vocabulary_\n",
    "    \n",
    "# # with open(\"yake_vocabulary.txt\", \"w\") as text_file:\n",
    "# #     text_file.write(str(yake_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representative Data Point\n",
    "If you use k-means, there’s a function that returns the coordinates of the cluster center. You can then calculate the cosine distance of all the vectors in your cluster to this center point to find which data point is closest to the center. You can think of this as the best example or most representative data point in your cluster. Convert this data point back to its original form to get a better understanding of which types of requirements exist in this cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FIXME - This code is pulled from the referenced WSD project lines of code from the roadmap doc\n",
    "\n",
    "# ### Look only at top 15 largest clusters ###\n",
    "\n",
    "# df_clusters = pd.DataFrame(system_df.groupby(['labels']).count()['ID']).sort_values(\n",
    "#     by='ID', ascending=False).reset_index()    \n",
    "# df_clusters.rename(columns = {'ID': 'count'}, inplace=True)\n",
    "# top15 = df_clusters['labels'][:15].tolist()\n",
    "# system_df = system_df[system_df['labels'].isin(top15)].copy().reset_index(drop = True)\n",
    "# df_clusters = df_clusters.merge(pd.DataFrame(system_df.groupby(['labels']).sum()['Elevated']),\n",
    "#                                 on='labels', how='left')\n",
    "# ### Find cluster centers ###\n",
    "\n",
    "# indices = []\n",
    "# example_tix_titles = []\n",
    "# example_tix_summaries = []\n",
    "# for label in top15:\n",
    "#     c = model.cluster_centers_[label]\n",
    "#     Y = np.array([np.array(x) for x in system_df['inference']])\n",
    "#     cosine_distances = []\n",
    "#     for row in Y:\n",
    "#         cosine_distances.append(spatial.distance.cosine(row, c))\n",
    "#     num_examples = min(int(df_clusters[df_clusters['labels']==label]['count'].values[0]), 3)\n",
    "#     indices.append([cosine_distances.index(sorted(cosine_distances)[i]) for i in range(num_examples)]) ## three examples closest to cluster center\n",
    "# for sublist in indices:\n",
    "#     example_tix_titles.append([str(system_df.iloc[i]['ID']) + ': ' + str(system_df.iloc[i]['Title']) for i in sublist])\n",
    "#     example_tix_summaries.append([utils.preprocess_text(str(system_df.iloc[i]['Brief_x0020_description']), keep_original=True, \n",
    "#                                                        display=True) for i in sublist])\n",
    "# temp_dict = {'labels': top15, 'example_tix_ids': example_tix_titles, 'example_tix_summaries':example_tix_summaries}\n",
    "# temp = pd.DataFrame(temp_dict).reset_index(drop=True)\n",
    "\n",
    "# system_df = system_df.merge(temp, how = 'left', on = 'labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
